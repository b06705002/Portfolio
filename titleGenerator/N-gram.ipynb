{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import choice\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import math\n",
    "from collections import Counter\n",
    "from nnsplit import NNSplit\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataNormalize(raw_path, new_article=None):\n",
    "\n",
    "    with open(raw_path, \"rb\") as fp:\n",
    "        train = pickle.load(fp)\n",
    "\n",
    "    token_set = []\n",
    "    for i in range(len(train)):\n",
    "        word_tokens = tokenizer.tokenize(train[i]['content'])\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            filtered_sentence.append(lemmatizer.lemmatize(w))\n",
    "        token_set.append(filtered_sentence)\n",
    "\n",
    "    if new_article != None:\n",
    "        word_tokens = tokenizer.tokenize(new_article)\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            filtered_sentence.append(lemmatizer.lemmatize(w))\n",
    "        token_set.append(filtered_sentence)\n",
    "\n",
    "    return token_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTfidf(token_set, header=None):\n",
    "    \n",
    "    IDF = {}\n",
    "    showed = {}\n",
    "    for i in range(len(token_set)):\n",
    "        showed = {}\n",
    "        for word in token_set[i]:\n",
    "            if word not in IDF:\n",
    "                IDF[word] = 1\n",
    "            else:\n",
    "                if word not in showed:\n",
    "                    showed[word] = 1\n",
    "                    IDF[word] += 1\n",
    "                else:\n",
    "                    continue\n",
    "    for key in IDF.keys():\n",
    "        IDF[key] = math.log(len(token_set)/IDF[key], 10)\n",
    "    IDF = {k: v for k, v in sorted(IDF.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    tfidf_arr = []\n",
    "    for i in range(len(token_set)):\n",
    "        TF = Counter(token_set[i])\n",
    "        TFIDF = {}\n",
    "        for key in TF.keys():\n",
    "            try:\n",
    "                TFIDF[key] = TF[key] * IDF[key]\n",
    "            except:\n",
    "                print(key)\n",
    "                \n",
    "        TFIDF = sorted(TFIDF.items(), key=lambda item: item[1], reverse=True)\n",
    "        tfidf_arr.append(TFIDF)\n",
    "    \n",
    "    return tfidf_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(arr, n):\n",
    "    \n",
    "    grams = []\n",
    "    for i in range(len(arr)):\n",
    "        if i < len(arr) - n + 1:\n",
    "            temp = tuple([arr[i+j] for j in range(n)])\n",
    "            grams.append(temp)\n",
    "            \n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nGramLM(data, n):\n",
    "    \n",
    "    nGramDict = {}\n",
    "    for i in range(len(data)):\n",
    "        trainData = ngrams(data[i], n)\n",
    "        for j in range(len(trainData)):\n",
    "            termTuple = trainData[j]\n",
    "            if termTuple not in nGramDict.keys():\n",
    "                nGramDict[termTuple] = 1\n",
    "            else:\n",
    "                nGramDict[termTuple] += 1\n",
    "    \n",
    "    return nGramDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseBigramWord(cfd, tfidf, key):\n",
    "    \n",
    "    candidateKey = []\n",
    "    for i in cfd.keys():\n",
    "        if i[0] == key:\n",
    "            candidateKey.append(i)\n",
    "            \n",
    "    pList = []\n",
    "    for i in candidateKey:\n",
    "        pList.append(cfd[i]*tfidf[i[1]])\n",
    "    \n",
    "    res = random.choices(population=candidateKey, weights=pList, k=1)\n",
    "    \n",
    "    return res[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chooseTrigramWord(cfd, tfidf, firstKey, secondKey):\n",
    "    \n",
    "    candidateKey = []\n",
    "    for i in cfd.keys():\n",
    "        if i[0] == firstKey and i[1] == secondKey:\n",
    "            candidateKey.append(i)\n",
    "            \n",
    "    pList = []\n",
    "    for i in candidateKey:\n",
    "        pList.append(cfd[i]*tfidf[i[2]])\n",
    "    \n",
    "    assert(len(pList) != 0)\n",
    "    \n",
    "    res = random.choices(population=candidateKey, weights=pList, k=1)\n",
    "    \n",
    "    return res[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateArticle(cfd, tfidf, word, n, num=10):\n",
    "    \n",
    "    arr = []\n",
    "    arr.append(word)\n",
    "    \n",
    "    if n == 2:\n",
    "        for i in range(num):\n",
    "            newWord = chooseBigramWord(cfd, tfidf, word)\n",
    "            arr.append(newWord)\n",
    "            word = newWord\n",
    "    \n",
    "    elif n == 3:\n",
    "        newWord = chooseBigramWord(cfd, tfidf, word)\n",
    "        arr.append(newWord)\n",
    "        firstWord = word\n",
    "        secondWord = newWord\n",
    "        for i in range(num-1):\n",
    "            newWord = chooseTrigramWord(cfd, tfidf, firstWord, secondWord)\n",
    "            arr.append(newWord)\n",
    "            firstWord = secondWord\n",
    "            secondWord = newWord\n",
    "            \n",
    "    else:\n",
    "        print(\"N-gram Error\")\n",
    "            \n",
    "    return arr[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(data, tfidf, artNum, n, senLen):\n",
    "    \n",
    "    tfidf = dict((x, y) for x, y in tfidf[artNum])\n",
    "    max_value = max(tfidf.values())\n",
    "    max_keys = [k for k, v in tfidf.items() if v == max_value]\n",
    "    lm = nGramLM([data[artNum]], n)\n",
    "    art = generateArticle(lm, tfidf, max_keys[0], n, senLen)\n",
    "    \n",
    "    return art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSentence(arr):\n",
    "    \n",
    "    text = \" \".join(arr)\n",
    "    splitter = NNSplit.load(\"en\")\n",
    "    splits = splitter.split([text])[0]\n",
    "        \n",
    "    return splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addFarmWord(title):\n",
    "    \n",
    "    farm_word = [\"I was stunned after reading it!!! \", \"The whole audience collapsed!!! \", \"I can't believe my eyes!!!\", \n",
    "                \"I watched it three times and still can't believe it!!! \", \"You must see to the end!!! \", \"My jaw dropped!!! \",\n",
    "                \"This is not true, is it!!! \", \"Let the experts fall through the glasses!!! \", \"I jumped up from the chair!!! \",\n",
    "                \"Unexpected news!!! \", \"This is really incredible!!! \", \"News that has never been revealed!!! \",\n",
    "                \"Frightened everyone!!! \",\"I guarantee you have never seen it!!! \", \"Unbelievable fact!!! \"]\n",
    "\n",
    "    res = random.choice(farm_word) + title\n",
    "    res = res[:-1] + \"!\"\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catchWord(path, text):\n",
    "    \n",
    "    token_set = dataNormalize(path, text)\n",
    "    tfidf = genTfidf(token_set)\n",
    "    n = 3\n",
    "    senLen = 20\n",
    "    res = project(token_set, tfidf, len(token_set)-1, n, senLen)\n",
    "    sentence = splitSentence(res)\n",
    "    \n",
    "    print(addFarmWord(str(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The whole audience collapsed!!! Nets In a separate deal Houston is trading LeVert and Rodions Kurucs from the Cavaliers via the Milwaukee Bucks an!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    rawPath = \"./SPORTS_Raw.pkl\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    text = \"The Houston Rockets are moving on from franchise superstar James Harden. They have traded the 31-year-old to the Brooklyn Nets as part of a three-team deal, the Nets announced on Thursday. In return for Harden, Houston is acquiring Caris LeVert and Rodions Kurucs from the Nets, Dante Exum from the Cleveland Cavaliers, three first-round picks from the Nets, one first-round pick from the Cavaliers via the Milwaukee Bucks, and four first-round pick swaps from the Nets. In a separate deal, Houston is trading LeVert and a second-round pick to the Indiana Pacers for guard and two-time All-Star Victor Oladipo, according to The Athletic's Shams Charania. Harden, an eight-time All-Star, was acquired by the Rockets from the Oklahoma City Thunder in 2012. While in Houston, he was voted the league's best player for the 2017-18 season and led the Rockets to the playoffs in all eight years. The postgame comments were the last of a string a of negative behavior from the disgruntled star, after arriving late to the team's training camp, and then being sidelined for four days and fined $50,000 by the NBA for violating the league's health and safety protocols days before the start of the season. The former MVP now reunites with former Thunder teammate Kevin Durant and perennial All-Star guard Kyrie Irving in Brooklyn.\"\n",
    "    catchWord(rawPath, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
