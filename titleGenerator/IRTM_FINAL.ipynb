{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"IRTM_FINAL.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Ao1-bQaN7eGJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610854686558,"user_tz":-480,"elapsed":11246,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"00e38048-c69d-49a9-b97e-f29e0d011e96"},"source":["!pip install nltk==3.4.5\r\n","import nltk.lm\r\n","import numpy as np\r\n","import pandas as pd\r\n","from keras.utils import to_categorical\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.models import Sequential\r\n","from keras.layers import LSTM, Dense, GRU, Embedding\r\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n","import re\r\n","import pickle"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting nltk==3.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 4.4MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.5) (1.15.0)\n","Building wheels for collected packages: nltk\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-cp36-none-any.whl size=1449905 sha256=e950e4785a33b42223231d9c747dab0bcba63292fe46ffb93322dceb295faccc\n","  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n","Successfully built nltk\n","Installing collected packages: nltk\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","Successfully installed nltk-3.4.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nI1hJIG8-FVe"},"source":["topic = \"SPORTS\"\r\n","with open('./drive/MyDrive/4rd1/DataMining/IRTM_Final/{}_Raw.pkl'.format(topic), \"rb\") as fp:\r\n","  train = pickle.load(fp)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBuWoD4C-tfX"},"source":["all_text = \"\"\r\n","N = len(train)\r\n","for i in range(N):\r\n","  all_text += train[i]['content']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"3QZ5jnVUZ9NI","executionInfo":{"status":"ok","timestamp":1610790854546,"user_tz":-480,"elapsed":689,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"2238835b-c178-4440-a638-d771021930df"},"source":["(train[0]['content'])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'When it comes to New York Jets players kneeling during the national anthem, team chairman Christopher Johnson is a standup guy.Johnson said the team wouldn’t fine any player for protesting, despite a new NFL owner-approved measure that enables teams to do so, according to reports.Wednesday’s rule change says\\xa0players must stand for the anthem or stay off the field. If any player kneels, his team would face a league fine. Teams can discipline individual players.“As I have in the past, I will support our players wherever we land as a team,” Johnson said in a statement, according to the New York Post. “Our focus is not on imposing any Club rules, fines, or restrictions. Instead we will continue to work closely with our players to constructively advance social justice issues that are important to us. I remain extremely proud of how we demonstrated unity last season as well as our players’ commitment to strengthening our communities.”Johnson elaborated on his pledge not to fine players\\xa0in an interview with Newsday.\\xa0“I do not like imposing any club-specific rules,” he said. “If somebody (on the Jets) takes a knee, that fine will be borne by the organization, by me, not the players.“I never want to put restrictions on the speech of our players. Do I prefer they stand? Of course. But I understand if they feel the need to protest. There are some big, complicated issues that we’re all struggling with, and our players are on the front lines.”Johnson’s stance could get interesting in the team’s hierarchy. He has been\\xa0designated as the acting owner while his brother, team owner Woody Johnson, serves as President Donald Trump’s ambassador to the United Kingdom, CBS Sports pointed out.Trump, who has repeatedly criticized demonstrating players, applauded the new NFL edict.\\xa0“You have to stand proudly for the national anthem or you shouldn’t be playing,” he said.The NFL Players Association criticized the league for failing to consult the union before the change, and vowed to carefully review the new policy.'"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"lE6Kk3g0-yHL"},"source":["data_text = all_text\r\n","def text_cleaner(text):\r\n","    # lower case text\r\n","    newString = text.lower()\r\n","    newString = re.sub(r\"'s\\b\",\"\",newString)\r\n","    # remove punctuations\r\n","    newString = re.sub(\"[^a-zA-Z]\", \" \", newString) \r\n","    long_words=[]\r\n","    # remove short word\r\n","    for i in newString.split():\r\n","        if len(i)>=3:                  \r\n","            long_words.append(i)\r\n","    return (\" \".join(long_words)).strip()\r\n","\r\n","# preprocess the text\r\n","data_new = text_cleaner(data_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnnWmUaCaBBM","executionInfo":{"status":"ok","timestamp":1610854777180,"user_tz":-480,"elapsed":766,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"ccb1197a-ad8a-461f-adf7-b1214cfd17aa"},"source":["import markovify\r\n","print(train[0]['content'])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["When it comes to New York Jets players kneeling during the national anthem, team chairman Christopher Johnson is a standup guy.Johnson said the team wouldn’t fine any player for protesting, despite a new NFL owner-approved measure that enables teams to do so, according to reports.Wednesday’s rule change says players must stand for the anthem or stay off the field. If any player kneels, his team would face a league fine. Teams can discipline individual players.“As I have in the past, I will support our players wherever we land as a team,” Johnson said in a statement, according to the New York Post. “Our focus is not on imposing any Club rules, fines, or restrictions. Instead we will continue to work closely with our players to constructively advance social justice issues that are important to us. I remain extremely proud of how we demonstrated unity last season as well as our players’ commitment to strengthening our communities.”Johnson elaborated on his pledge not to fine players in an interview with Newsday. “I do not like imposing any club-specific rules,” he said. “If somebody (on the Jets) takes a knee, that fine will be borne by the organization, by me, not the players.“I never want to put restrictions on the speech of our players. Do I prefer they stand? Of course. But I understand if they feel the need to protest. There are some big, complicated issues that we’re all struggling with, and our players are on the front lines.”Johnson’s stance could get interesting in the team’s hierarchy. He has been designated as the acting owner while his brother, team owner Woody Johnson, serves as President Donald Trump’s ambassador to the United Kingdom, CBS Sports pointed out.Trump, who has repeatedly criticized demonstrating players, applauded the new NFL edict. “You have to stand proudly for the national anthem or you shouldn’t be playing,” he said.The NFL Players Association criticized the league for failing to consult the union before the change, and vowed to carefully review the new policy.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"app-uVdIapVA","executionInfo":{"status":"ok","timestamp":1610855484412,"user_tz":-480,"elapsed":646,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"5f395175-09e2-4b4b-c4c4-0149f646df3c"},"source":["text_model = markovify.Text(train[0]['content'], state_size=2)\r\n","print(train[0].keys())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dict_keys(['category', 'headline', 'authors', 'link', 'short_description', 'date', 'content'])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9uT4rnMcYB-","executionInfo":{"status":"ok","timestamp":1610855485965,"user_tz":-480,"elapsed":754,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"0c689cfa-8220-4c07-d550-5df284dcf24b"},"source":["print(train[0]['headline'])\r\n","for i in range(5):\r\n","  print(text_model.make_sentence())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Jets Chairman Christopher Johnson Won't Fine Players For Anthem Protests\n","“You have to stand proudly for the national anthem or stay off the field.\n","Teams can discipline individual players.“As I have in the past, I will support our players are on the front lines.”Johnson’s stance could get interesting in the team’s hierarchy.\n","Teams can discipline individual players.“As I have in the past, I will support our players are on the front lines.”Johnson’s stance could get interesting in the team’s hierarchy.\n","Teams can discipline individual players.“As I have in the past, I will support our players are on the front lines.”Johnson’s stance could get interesting in the team’s hierarchy.\n","“Our focus is not on imposing any club-specific rules,” he said.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RKrGg-70-0UP","executionInfo":{"status":"ok","timestamp":1608045523295,"user_tz":-480,"elapsed":3145,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"ffcad022-8916-480e-f1a5-0d12caf6c674"},"source":["def create_seq(text):\r\n","    length = 30\r\n","    sequences = list()\r\n","    for i in range(length, len(text)):\r\n","        # select sequence of tokens\r\n","        seq = text[i-length:i+1]\r\n","        # store\r\n","        sequences.append(seq)\r\n","    print('Total Sequences: %d' % len(sequences))\r\n","    return sequences\r\n","\r\n","# create sequences   \r\n","sequences = create_seq(data_new)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Sequences: 9120058\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4f1g-CMi-2Tf"},"source":["# create a character mapping index\r\n","chars = sorted(list(set(data_new)))\r\n","mapping = dict((c, i) for i, c in enumerate(chars))\r\n","\r\n","def encode_seq(seq):\r\n","    sequences = list()\r\n","    for line in seq:\r\n","        # integer encode line\r\n","        encoded_seq = [mapping[char] for char in line]\r\n","        # store\r\n","        sequences.append(encoded_seq)\r\n","    return sequences\r\n","\r\n","# encode the sequences\r\n","sequences = encode_seq(sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXSI16xj-3s3","executionInfo":{"status":"ok","timestamp":1608045596750,"user_tz":-480,"elapsed":31428,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"7602687a-4b83-4e4f-b47d-16562cf94d30"},"source":["from sklearn.model_selection import train_test_split\r\n","\r\n","# vocabulary size\r\n","vocab = len(mapping)\r\n","sequences = np.array(sequences)\r\n","# create X and y\r\n","X, y = sequences[:,:-1], sequences[:,-1]\r\n","# one hot encode y\r\n","y = to_categorical(y, num_classes=vocab)\r\n","# create train and validation sets\r\n","X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\r\n","\r\n","print('Train shape:', X_tr.shape, 'Val shape:', X_val.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train shape: (8208052, 30) Val shape: (912006, 30)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6nErVDBM-8b-","outputId":"53efc9ff-93f4-4302-bf39-f1f16fb22b51"},"source":["# define model\r\n","model = Sequential()\r\n","model.add(Embedding(vocab, 50, input_length=30, trainable=True))\r\n","model.add(GRU(150, recurrent_dropout=0.1, dropout=0.1))\r\n","model.add(Dense(vocab, activation='softmax'))\r\n","print(model.summary())\r\n","\r\n","# compile the model\r\n","model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\r\n","# fit the model\r\n","model.fit(X_tr, y_tr, epochs=10, batch_size = 3000,verbose=1, validation_data=(X_val, y_val))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, 30, 50)            1350      \n","_________________________________________________________________\n","gru (GRU)                    (None, 150)               90900     \n","_________________________________________________________________\n","dense (Dense)                (None, 27)                4077      \n","=================================================================\n","Total params: 96,327\n","Trainable params: 96,327\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","Epoch 1/10\n","2500/2737 [==========================>...] - ETA: 8:06 - loss: 1.8324 - acc: 0.4485"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V2bkwQTP_BlX"},"source":["def generate_seq(model, mapping, seq_length, seed_text, n_chars):\r\n","    in_text = seed_text\r\n","    # generate a fixed number of characters\r\n","    for _ in range(n_chars):\r\n","        # encode the characters as integers\r\n","        encoded = [mapping[char] for char in in_text]\r\n","        # truncate sequences to a fixed length\r\n","        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\r\n","        # predict character\r\n","        yhat = model.predict_classes(encoded, verbose=0)\r\n","        # reverse map integer to character\r\n","        out_char = ''\r\n","        for char, index in mapping.items():\r\n","            if index == yhat:\r\n","                out_char = char\r\n","                break\r\n","        # append to input\r\n","        in_text += char\r\n","    return in_text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWpAq9tU_FEl"},"source":["t = generate_seq(model, mapping, 15, \"\", 100)\r\n","print(t)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3wl7Y1_KjK6","executionInfo":{"status":"ok","timestamp":1610784781172,"user_tz":-480,"elapsed":10669,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"99aaaa54-8c2c-4953-8cf5-deef3f47c478"},"source":["!git clone https://github.com/huggingface/pytorch-transformers.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-transformers'...\n","remote: Enumerating objects: 59307, done.\u001b[K\n","remote: Total 59307 (delta 0), reused 0 (delta 0), pack-reused 59307\u001b[K\n","Receiving objects: 100% (59307/59307), 44.34 MiB | 24.69 MiB/s, done.\n","Resolving deltas: 100% (41758/41758), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpT4NktWKnkn","executionInfo":{"status":"ok","timestamp":1610733003473,"user_tz":-480,"elapsed":619,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"56fbdc23-2114-4fbd-d7fc-cdb484d68a5b"},"source":["!python pytorch-transformers/examples/run_generation.py --model_type=gpt2 --length=100 --model_name_or_path=gpt2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["python3: can't open file 'pytorch-transformers/examples/run_generation.py': [Errno 2] No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G2BDCM0ceile","executionInfo":{"status":"ok","timestamp":1610788549434,"user_tz":-480,"elapsed":1940,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"db29193f-d0cd-4b1c-ddc9-6ed6d9c4fff9"},"source":["!git clone https://github.com/nlpyang/pytorch-transformers.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-transformers'...\n","remote: Enumerating objects: 6469, done.\u001b[K\n","remote: Total 6469 (delta 0), reused 0 (delta 0), pack-reused 6469\u001b[K\n","Receiving objects: 100% (6469/6469), 3.60 MiB | 12.34 MiB/s, done.\n","Resolving deltas: 100% (4641/4641), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vd94QjxCerx-","executionInfo":{"status":"ok","timestamp":1610853776899,"user_tz":-480,"elapsed":502246,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"8ec456ae-0b24-4355-aaeb-6feb8f044561"},"source":["# 預測出一整個文章\r\n","!python pytorch-transformers/examples/run_generation.py --model_type=gpt2 --length=60 --model_name_or_path=gpt2"],"execution_count":null,"outputs":[{"output_type":"stream","text":["01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /tmp/tmpt9z2e0c9\n","100% 1042301/1042301 [00:00<00:00, 12378324.84B/s]\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpt9z2e0c9 to cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpt9z2e0c9\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /tmp/tmp9q12cisi\n","100% 456318/456318 [00:00<00:00, 6674302.78B/s]\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmp9q12cisi to cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmp9q12cisi\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json not found in cache or force_download set to True, downloading to /tmp/tmpj5so_hwe\n","100% 665/665 [00:00<00:00, 489421.33B/s]\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpj5so_hwe to cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpj5so_hwe\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n","01/17/2021 03:14:35 - INFO - pytorch_transformers.modeling_utils -   Model config {\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 50256,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 50256,\n","  \"finetuning_task\": null,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 768,\n","  \"n_head\": 12,\n","  \"n_layer\": 12,\n","  \"n_positions\": 1024,\n","  \"num_labels\": 1,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"pruned_heads\": {},\n","  \"resid_pdrop\": 0.1,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"torchscript\": false,\n","  \"vocab_size\": 50257\n","}\n","\n","01/17/2021 03:14:36 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpaok5cqvo\n","100% 548118077/548118077 [00:08<00:00, 62724022.13B/s]\n","01/17/2021 03:14:44 - INFO - pytorch_transformers.file_utils -   copying /tmp/tmpaok5cqvo to cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n","01/17/2021 03:14:46 - INFO - pytorch_transformers.file_utils -   creating metadata file for /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n","01/17/2021 03:14:46 - INFO - pytorch_transformers.file_utils -   removing temp file /tmp/tmpaok5cqvo\n","01/17/2021 03:14:46 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n","Namespace(device=device(type='cpu'), length=60, model_name_or_path='gpt2', model_type='gpt2', n_gpu=0, no_cuda=False, padding_text='', prompt='', seed=42, temperature=1.0, top_k=0, top_p=0.9)\n","Model prompt >>> When it comes to New York Jets players kneeling during the national anthem, team chairman Christopher Johnson is a standup guy.Johnson said the team wouldn’t fine any player for protesting, despite a new NFL owner-approved measure that enables teams to do so, according to reports.Wednesday’s rule change says players must stand for the anthem or stay off the field. If any player kneels, his team would face a league fine. Teams can discipline individual players.“As I have in the past, I will support our players wherever we land as a team,” Johnson said in a statement, according to the New York Post. “Our focus is not on imposing any Club rules, fines, or restrictions. Instead we will continue to work closely with our players to constructively advance social justice issues that are important to us. I remain extremely proud of how we demonstrated unity last season as well as our players’ commitment to strengthening our communities.”Johnson elaborated on his pledge not to fine players in an interview with Newsday. “I do not like imposing any club-specific rules,” he said. “If somebody (on the Jets) takes a knee, that fine will be borne by the organization, by me, not the players.“I never want to put restrictions on the speech of our players. Do I prefer they stand? Of course. But I understand if they feel the need to protest. There are some big, complicated issues that we’re all struggling with, and our players are on the front lines.”Johnson’s stance could get interesting in the team’s hierarchy. He has been designated as the acting owner while his brother, team owner Woody Johnson, serves as President Donald Trump’s ambassador to the United Kingdom, CBS Sports pointed out.Trump, who has repeatedly criticized demonstrating players, applauded the new NFL edict. “You have to stand proudly for the national anthem or you shouldn’t be playing,” he said.The NFL Players Association criticized the league for failing to consult the union before the change, and vowed to carefully review the new policy.\n","100% 60/60 [02:36<00:00,  2.60s/it]\n","And that means no major drama should ensue when the kneeling protest goes ahead. In a statement on the \"Star Wars: The Force Awakens\" trailer, released earlier this month, the group argued that if game after game begins, fans would still be protesting, but it was inevitable they would arrive alone\n","Model prompt >>> Traceback (most recent call last):\n","  File \"pytorch-transformers/examples/run_generation.py\", line 195, in <module>\n","    main()\n","  File \"pytorch-transformers/examples/run_generation.py\", line 171, in main\n","    raw_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n","KeyboardInterrupt\n","^C\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KREf-H9tVenL","executionInfo":{"status":"ok","timestamp":1610853272326,"user_tz":-480,"elapsed":11166,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"55bef795-5a50-436a-dd90-d86370e2028b"},"source":["!pip install pytorch_transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch_transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n","\r\u001b[K     |█▉                              | 10kB 14.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20kB 11.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 40kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 51kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 61kB 4.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 92kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 133kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 143kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 153kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 163kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 174kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.7.0+cu101)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.41.1)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/4c/e81c2f215e93a6cb5efdaa991669e3ef1ec6ecfe4407c582c3dfc7d2c281/boto3-1.16.56-py2.py3-none-any.whl (130kB)\n","\u001b[K     |████████████████████████████████| 133kB 12.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.19.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 13.3MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.23.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.7.4.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.0.0)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting botocore<1.20.0,>=1.19.56\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/72/c904c62945127699aac8aa5bbd508ec851d55da46aaf1072c061de3eb6fa/botocore-1.19.56-py2.py3-none-any.whl (7.2MB)\n","\u001b[K     |████████████████████████████████| 7.2MB 20.5MB/s \n","\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/43/4b4a1b26eb03a429a4c37ca7fdf369d938bd60018fc194e94b8379b0c77c/s3transfer-0.3.4-py2.py3-none-any.whl (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 6.1MB/s \n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.56->boto3->pytorch_transformers) (2.8.1)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=0fd8b159756dc54e165d742d2c9ff4c8390d417e5660815ab1857853187a1ba8\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","\u001b[31mERROR: botocore 1.19.56 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: sacremoses, jmespath, botocore, s3transfer, boto3, sentencepiece, pytorch-transformers\n","Successfully installed boto3-1.16.56 botocore-1.19.56 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.3.4 sacremoses-0.0.43 sentencepiece-0.1.95\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"63-xuQG2VTwK","executionInfo":{"status":"ok","timestamp":1610853233449,"user_tz":-480,"elapsed":2310,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"f6f7a1d7-f8a6-4cce-e9ad-b078a0e2319b"},"source":["!git clone https://github.com/nlpyang/pytorch-transformers.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'pytorch-transformers'...\n","remote: Enumerating objects: 6469, done.\u001b[K\n","remote: Total 6469 (delta 0), reused 0 (delta 0), pack-reused 6469\u001b[K\n","Receiving objects: 100% (6469/6469), 3.60 MiB | 18.26 MiB/s, done.\n","Resolving deltas: 100% (4641/4641), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"W2Qe0NI9KyAL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610788788425,"user_tz":-480,"elapsed":7107,"user":{"displayName":"劉品枘","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGTWtnR54ZmQ-vM-eZ8SyhW9SiyWi2MuttRLltUQ=s64","userId":"10268024118796963261"}},"outputId":"48762809-656a-4205-874e-2e693c5abc9b"},"source":["# 預測下一個字\r\n","\r\n","import torch\r\n","from pytorch_transformers import GPT2Tokenizer, GPT2LMHeadModel\r\n","\r\n","# 加载预训练模型tokenizer (vocabulary)\r\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\r\n","\r\n","# 对文本输入进行编码\r\n","text = \"What is the fastest car in the\"\r\n","text = \"I want to fuck\"\r\n","indexed_tokens = tokenizer.encode(text)\r\n","\r\n","# 在PyTorch张量中转换indexed_tokens\r\n","tokens_tensor = torch.tensor([indexed_tokens])\r\n","\r\n","# 加载预训练模型 (weights)\r\n","model = GPT2LMHeadModel.from_pretrained('gpt2')\r\n","\r\n","#将模型设置为evaluation模式，关闭DropOut模块\r\n","model.eval()\r\n","\r\n","# 如果你有GPU，把所有东西都放在cuda上\r\n","# tokens_tensor = tokens_tensor.to('cuda')\r\n","# model.to('cuda')\r\n","\r\n","# 预测所有的tokens\r\n","with torch.no_grad():\r\n","    outputs = model(tokens_tensor)\r\n","    predictions = outputs[0]\r\n","\r\n","# 得到预测的单词\r\n","predicted_index = torch.argmax(predictions[0, -1, :]).item()\r\n","predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\r\n","\r\n","# 打印预测单词\r\n","print(predicted_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[" I want to fuck you\n"],"name":"stdout"}]}]}